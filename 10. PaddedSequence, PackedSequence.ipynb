{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 14])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(14)\n",
    "b = torch.ones(7)\n",
    "c = torch.ones(11)\n",
    "d = torch.ones(9)\n",
    "data_arr = [a, b, c, d]\n",
    "\n",
    "padded_seq = pad_sequence(data_arr, batch_first=True)\n",
    "padded_seq.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- padded_seq = [Batch, seq_len, input_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 14, 300])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(14, 300)  #word2vec embedding\n",
    "b = torch.ones(7, 300)\n",
    "c = torch.ones(11, 300)\n",
    "d = torch.ones(9, 300)\n",
    "data_word2vec = [a,b,c,d]\n",
    "padded_seq_word2vec = pad_sequence(data_word2vec, batch_first=True)\n",
    "padded_seq_word2vec.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pack_sequence : zero padding하여 불필요한 zeros가 추가 되는 것이 꺼림찍 할 때 사용할 수 있는 torch 자료구조\n",
    "- 주어지는 input (list of Tensor)는 길이에 따른 내림차순으로 정렬이 되어있어야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 7, 11, 9]\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(item) for item in data_arr]\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Sort by descending lengths\n",
    "sorted_idx = sorted(range(len(lengths)), key=lengths.__getitem__, reverse=True)\n",
    "sorted_data = [data_arr[idx] for idx in sorted_idx]\n",
    "\n",
    "# Check converted result\n",
    "for sequence in sorted_data:\n",
    "    print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1.]), batch_sizes=tensor([4, 4, 4, 4, 4, 4, 4, 3, 3, 2, 2, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)\n"
     ]
    }
   ],
   "source": [
    "packed_seq = pack_sequence(sorted_data)\n",
    "print(packed_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 7, 11, 9]\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(item) for item in data_word2vec]\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by descending lengths\n",
    "sorted_idx = sorted(range(len(lengths)), key=lengths.__getitem__, reverse=True)\n",
    "sorted_data_w2v = [data_word2vec[idx] for idx in sorted_idx]\n",
    "\n",
    "packed_seq_w2v = pack_sequence(sorted_data_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nn.RNN\n",
    "    - input_size: The number of expected features in the input `x`\n",
    "    - hidden_size: The number of features in the hidden state `h`\n",
    "    - num_layers: Number of recurrent layers. \n",
    "    - nonlinearity: Can be either ``'tanh'`` or ``'relu'``. Default: ``'tanh'``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_size=300, hidden_size=16, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 14, 16])\n",
      "torch.Size([1, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "outputs, state = rnn(padded_seq_word2vec) # seq_length가 모두 동일하게 14로 setting\n",
    "print(outputs.shape) # shape (batch, seq_len, num_directions * hidden_size)\n",
    "print(state.shape) # shape (num_layers * num_directions, batch, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, state = rnn(packed_seq_w2v) # seq_length가 모두 [14, 11, 9, 7]로 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([41, 16])\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 3, 3, 2, 2, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0].shape) # 14, 11, 9, 7 이 모두 합해져서 [41, 16]으로 나옴\n",
    "print(outputs[1]) # 이것을 어떻게 다시 정상으로 풀어야할지 힌트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape # 마지막 state는 따로 unpack 할 필요없이 한번에 나옴 -> 마지막 state만 쓸 때는 이것만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_unpacked, output_lengths = pad_packed_sequence(outputs, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 14, 16])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_unpacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 11,  9,  7])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.9871, -0.2388, -0.0806, -0.4983, -0.9996,  0.9938, -0.6307,\n",
       "         -0.9868, -0.6814,  0.9936, -0.9493,  0.9176,  0.9459,  0.8890,  0.8931],\n",
       "        [ 1.0000,  0.9836, -0.2090,  0.0439, -0.9148, -0.9993,  0.9932, -0.6368,\n",
       "         -0.9805, -0.9291,  0.9959, -0.9731,  0.8812,  0.9570,  0.8930,  0.7768],\n",
       "        [ 1.0000,  0.9829, -0.1926,  0.0745, -0.9257, -0.9992,  0.9928, -0.6471,\n",
       "         -0.9819, -0.9195,  0.9953, -0.9686,  0.8698,  0.9514,  0.9169,  0.8340],\n",
       "        [ 1.0000,  0.9827, -0.1981,  0.0958, -0.9243, -0.9992,  0.9928, -0.6563,\n",
       "         -0.9819, -0.9197,  0.9953, -0.9683,  0.8657,  0.9509,  0.9174,  0.8303],\n",
       "        [ 1.0000,  0.9826, -0.2028,  0.1013, -0.9240, -0.9992,  0.9927, -0.6519,\n",
       "         -0.9816, -0.9195,  0.9953, -0.9680,  0.8650,  0.9506,  0.9176,  0.8295],\n",
       "        [ 1.0000,  0.9826, -0.2054,  0.1011, -0.9236, -0.9993,  0.9928, -0.6506,\n",
       "         -0.9816, -0.9192,  0.9953, -0.9680,  0.8647,  0.9503,  0.9179,  0.8298],\n",
       "        [ 1.0000,  0.9826, -0.2060,  0.1007, -0.9234, -0.9993,  0.9928, -0.6504,\n",
       "         -0.9816, -0.9192,  0.9953, -0.9680,  0.8645,  0.9503,  0.9180,  0.8299],\n",
       "        [ 1.0000,  0.9826, -0.2061,  0.1005, -0.9234, -0.9993,  0.9928, -0.6504,\n",
       "         -0.9816, -0.9192,  0.9953, -0.9680,  0.8645,  0.9502,  0.9180,  0.8299],\n",
       "        [ 1.0000,  0.9826, -0.2060,  0.1004, -0.9234, -0.9993,  0.9928, -0.6504,\n",
       "         -0.9816, -0.9192,  0.9953, -0.9680,  0.8645,  0.9503,  0.9180,  0.8299],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_unpacked[2, :, :] # 9개 이후는 zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-torch] *",
   "language": "python",
   "name": "conda-env-.conda-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
